# candle-video

[![License](https://img.shields.io/badge/license-Apache%202.0-blue?style=flat-square)](https://github.com/FerrisMind/candle-video/blob/main/LICENSE)

**candle-video** is a Rust library for video generation using AI models, built on top of the [Candle](https://github.com/huggingface/candle) ML framework. It provides high-performance inference for state-of-the-art video generation models.

## Demonstration

| Model | Video | Prompt |
| :--- | :---: | :--- |
| **LTX-Video-0.9.5** | ![Waves and Rocks](https://raw.githubusercontent.com/FerrisMind/candle-video/main/examples/ltx-video/output/0.9.5/Waves_and_Rocks.gif) | *The waves crash against the jagged rocks of the shoreline, sending spray high into the air. The rocks are a dark gray color, with sharp edges and deep crevices. The water is a clear blue-green, with white foam where the waves break against the rocks. The sky is a light gray, with a few white clouds dotting the horizon.* |
| **LTX-Video-0.9.8-2b-distilled** | ![woman_with_blood](https://raw.githubusercontent.com/FerrisMind/candle-video/main/examples/ltx-video/output/0.9.8/woman_with_blood.gif) | *A woman with blood on her face and a white tank top looks down and to her right, then back up as she speaks. She has dark hair pulled back, light skin, and her face and chest are covered in blood. The camera angle is a close-up, focused on the woman's face and upper torso. The lighting is dim and blue-toned, creating a somber and intense atmosphere. The scene appears to be from a movie or TV show.* |
| **Stable Diffusion Video** | *in the process of implementation...* | *in the process of implementation...* |
| **Wan2.1/2.2** | *in plans...* |  *in plans...* |


More examples of generation can be found here in [examples](examples/)

ðŸŒ **[Ð ÑƒÑÑÐºÐ°Ñ Ð²ÐµÑ€ÑÐ¸Ñ (Russian)](README.RU.md)**

## Supported Models

- **[LTX-Video](https://huggingface.co/Lightricks/LTX-Video)** â€” Text-to-video generation using DiT (Diffusion Transformer) architecture
  - Transformer-based diffusion model
  - T5-XXL text encoder (with GGUF quantization support)
  - 3D VAE for video encoding/decoding
  - Flow Matching scheduler

## Features

- ðŸš€ **High Performance** â€” Native Rust with GPU acceleration via CUDA/cuDNN
- ðŸ’¾ **Memory Efficient** â€” BF16 inference, VAE tiling/slicing, GGUF quantized text encoders
- ðŸ”§ **Flexible** â€” Run on CPU or GPU, with optional Flash Attention
- ðŸ“¦ **Standalone** â€” No Python runtime required in production

### Hardware Acceleration

| Feature | Description |
|---------|-------------|
| `cuda` | CUDA backend for NVIDIA GPUs |
| `cudnn` | cuDNN for faster convolutions |
| `flash-attn` | Flash Attention v2 for efficient attention |
| `mkl` | Intel MKL for optimized CPU operations (x86_64) |
| `accelerate` | Apple Accelerate for Metal (macOS) |
| `nccl` | Multi-GPU support via NCCL |

## Installation

### Prerequisites

- Rust 1.82+ (edition 2024)
- CUDA Toolkit 12.x (for GPU acceleration)
- cuDNN 8.x/9.x (optional, for faster convolutions)

### Add to your project

```toml
[dependencies]
candle-video = { git = "https://github.com/FerrisMind/candle-video" }
```

### Build with GPU support

```bash
# Default build (CUDA + cuDNN + Flash Attention)
cargo build --release

# CPU-only build
cargo build --release --no-default-features

# With specific features
cargo build --release --features "cudnn,flash-attn"
```

## Quick Start

### LTX-Video: Text-to-Video Generation

#### 1. Automatic usage (Recommended)
Weights will be automatically downloaded from [oxide-lab/LTX-Video-0.9.5](https://huggingface.co/oxide-lab/LTX-Video-0.9.5).

```bash
cargo run --example ltx-video --release --features flash-attn,cudnn -- \
    --prompt "A serene mountain lake at sunset, photorealistic, 4k"
```

#### 2. Manual usage (Local weights)
If you already have weights, provide the path:

```bash
cargo run --example ltx-video --release --features flash-attn,cudnn -- \
    --local-weights ./models/ltx-video \
    --prompt "A cat playing with a ball of yarn" \
    --vae-tiling
```
# Fast preview (384x256, 25 frames)
```bash
cargo run --example ltx-video --release --features flash-attn,cudnn -- \
    --local-weights ./models/ltx-video \
    --prompt "A futuristic cityscape with flying cars" \
    --height 256 \
    --width 384 \
    --num-frames 25 \
    --steps 20
```
# Low VRAM mode (with VAE tiling)
```bash
cargo run --example ltx-video --release --features flash-attn,cudnn -- \
    --local-weights ./models/ltx-video \
    --prompt "A majestic eagle soaring over snow-capped mountains" \
    --vae-tiling --vae-slicing
```

See [examples/ltx-video](examples/ltx-video/README.md) for more details.

### CLI Options

| Argument | Default | Description |
|----------|---------|-------------|
| `--prompt` | "A video of a cute cat..." | Text prompt for generation |
| `--negative-prompt` | "" | Negative prompt |
| `--height` | 512 | Video height (must be divisible by 32) |
| `--width` | 768 | Video width (must be divisible by 32) |
| `--num-frames` | 97 | Number of frames (should be 8n + 1) |
| `--steps` | (from version config) | Diffusion steps (40 for 0.9.5, 8 for distilled) |
| `--guidance-scale` | (from version config) | Classifier-free guidance scale |
| `--ltxv-version` | "0.9.5" | Model version (0.9.5, 0.9.6-distilled, 0.9.8-2b-distilled, etc.) |
| `--local-weights` | (None) | Path to local weights (auto-downloads if not set) |
| `--output-dir` | "output" | Directory to save results |
| `--seed` | random | Random seed for reproducibility |
| `--vae-tiling` | false | Enable VAE tiling for memory efficiency |
| `--vae-slicing` | false | Enable VAE batch slicing |
| `--frames` | false | Save individual PNG frames (disables GIF) |
| `--cpu` | false | Run on CPU instead of GPU |
| `--model-id` | "Lightricks/LTX-Video" | HF model ID (for tokenizer download) |
| `--use-bf16-t5` | false | Use BF16 T5 instead of GGUF quantized |
| `--unified-weights` | (None) | Path to unified safetensors file (official LTX format) |

### Library Usage

```rust
use candle_core::{Device, DType};
use candle_video::models::ltx_video::{
    LtxVideoTransformer3DModel,
    AutoencoderKLLtxVideo,
    FlowMatchEulerDiscreteScheduler,
    loader::WeightLoader,
};

fn main() -> anyhow::Result<()> {
    let device = Device::new_cuda(0)?;
    let dtype = DType::BF16;
    
    // Load Transformer
    let loader = WeightLoader::new(device.clone(), dtype);
    let vb = loader.load_single("path/to/transformer.safetensors")?;
    let config = LtxVideoTransformer3DModelConfig::default();
    let transformer = LtxVideoTransformer3DModel::new(&config, vb)?;
    
    // Load VAE
    let vae_vb = loader.load_single("path/to/vae.safetensors")?;
    let mut vae = AutoencoderKLLtxVideo::new(
        AutoencoderKLLtxVideoConfig::default(),
        vae_vb
    )?;
    
    // Enable memory optimizations
    vae.use_tiling = true;
    vae.use_slicing = true;
    
    // ... setup pipeline and generate
    Ok(())
}
```

## Project Structure

```
candle-video/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs              # Library entry point
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ ltx_video/      # LTX-Video model components
â”‚   â”‚   â”‚   â”œâ”€â”€ ltx_transformer.rs    # DiT transformer
â”‚   â”‚   â”‚   â”œâ”€â”€ vae.rs                # 3D VAE
â”‚   â”‚   â”‚   â”œâ”€â”€ text_encoder.rs       # T5 text encoder
â”‚   â”‚   â”‚   â”œâ”€â”€ quantized_t5_encoder.rs # GGUF T5 encoder
â”‚   â”‚   â”‚   â”œâ”€â”€ scheduler.rs          # Flow matching scheduler
â”‚   â”‚   â”‚   â”œâ”€â”€ t2v_pipeline.rs       # Text-to-video pipeline
â”‚   â”‚   â”‚   â””â”€â”€ loader.rs             # Weight loading utilities
â”‚   â”‚   â””â”€â”€ svd/            # Stable Video Diffusion components
â”‚   â”‚       â”œâ”€â”€ unet/       # UNet architecture
â”‚   â”‚       â”œâ”€â”€ vae/        # Temporal VAE
â”‚   â”‚       â”œâ”€â”€ clip.rs     # CLIP image encoder
â”‚   â”‚       â”œâ”€â”€ pipeline.rs # Generation pipeline
â”‚   â”‚       â””â”€â”€ scheduler.rs# EulerA scheduler
â”‚   â””â”€â”€ utils/              # Utilities
â”œâ”€â”€ examples/               # Usage examples (run with --example)
â”‚   â”œâ”€â”€ ltx_video/          # Text-to-video example
â”‚   â”‚   â”œâ”€â”€ main.rs         # Entry point
â”‚   â”‚   â””â”€â”€ README.md       # Detailed guide
â”‚   â””â”€â”€ verify/             # Verification and debug tools
â”œâ”€â”€ scripts/                # Python verification scripts
â”œâ”€â”€ tests/                  # Integration tests
â”œâ”€â”€ prebuilt/               # Prebuilt kernels (optional)
â””â”€â”€ tp/                     # Third-party submodules
```

## Model Weights

### LTX-Video

Download from [Lightricks/LTX-Video](https://huggingface.co/Lightricks/LTX-Video):

```bash
# Using huggingface-cli
huggingface-cli download Lightricks/LTX-Video --local-dir ./models/ltx-video

# For GGUF T5 encoder (memory efficient)
# Download t5-v1_1-xxl-encoder-Q5_K_M.gguf
```

**Required weight files:**
- `transformer/diffusion_pytorch_model.safetensors` â€” DiT model
- `vae/diffusion_pytorch_model.safetensors` â€” 3D VAE
- `text_encoder_gguf/t5-v1_1-xxl-encoder-Q5_K_M.gguf` â€” Quantized T5
- `tokenizer/tokenizer.json` â€” T5 tokenizer

## Memory Optimization

For limited VRAM, enable these options:

```bash
# VAE tiling - processes image in tiles
--vae-tiling

# VAE slicing - processes batches sequentially
--vae-slicing

# Lower resolution
--height 256 --width 384

# Fewer frames
--num-frames 25
```

**Approximate VRAM requirements (512x768, 97 frames):**
- Full model: ~8-12GB
- With VAE tiling: ~8GB
- With GGUF T5: saves ~8GB

## Comparison with PyTorch/diffusers

| Feature | candle-video | diffusers (Python) |
|---------|-------------|-------------------|
| Runtime | Rust native | Python + PyTorch |
| Startup | ~2 seconds | ~15-30 seconds |
| Binary size | ~50MB | ~2GB+ (with deps) |
| VRAM usage | Optimized | Standard |
| Deployment | Single binary | Python environment |

## Common Issues

### CUDA not found

```bash
# Ensure CUDA is in PATH
export PATH=/usr/local/cuda/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
```

### cuDNN errors on Windows

Copy and rename these DLLs to PATH:
- `nvcuda.dll` â†’ `cuda.dll`
- `cublas64_12.dll` â†’ `cublas.dll`
- `curand64_10.dll` â†’ `curand.dll`

### Out of Memory

Try reducing resolution, frames, or enabling VAE tiling:
```bash
--height 256 --width 384 --num-frames 25 --vae-tiling
```

## Contributing

Contributions are welcome! Please open an issue or pull request.

## License

Licensed under the Apache License, Version 2.0. See [LICENSE](LICENSE) for details.

## Acknowledgments

- [Candle](https://github.com/huggingface/candle) â€” Minimalist ML framework for Rust
- [Lightricks LTX-Video](https://huggingface.co/Lightricks/LTX-Video) â€” Original LTX-Video model
- [Stability AI](https://stability.ai/) â€” Stable Video Diffusion
- [diffusers](https://github.com/huggingface/diffusers) â€” Reference implementation
